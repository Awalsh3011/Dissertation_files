# -*- coding: utf-8 -*-
"""Dissertation code - normal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jsND4JZpkX9d4azQk6JzID4slo2pukfl
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn import metrics
import io
import pickle as pkl
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import confusion_matrix,accuracy_score,cohen_kappa_score,classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler
from scipy.special import boxcox1p
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
# %matplotlib inline
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import roc_auc_score

from google.colab import files
uploaded = files.upload()

"""## **General**"""

data = pd.read_csv(io.BytesIO(uploaded['online_shoppers_intention.csv']))
print(data)

print(data.shape)
print(data.info())

data.head()

data.dtypes

print(data.select_dtypes(include=['object']).dtypes)
print(data.select_dtypes(include=['int64']).dtypes)
print(data.select_dtypes(include=['float64']).dtypes)
print(data.select_dtypes(include=['bool']).dtypes)

data. isnull()

data.isnull().sum()

print(data['Revenue'].value_counts())
fig = plt.figure(figsize=(10,10)) 
fig_dims = (3, 2)

plt.subplot2grid(fig_dims, (0, 0))
data['Revenue'].value_counts().plot(kind='bar', title='Revenue')

print(data['Weekend'].value_counts())
fig = plt.figure(figsize=(10,10)) 
fig_dims = (3, 2)

plt.subplot2grid(fig_dims, (0, 0))
data['Weekend'].value_counts().plot(kind='bar',title='Weekend')

print(data['Month'].value_counts())
fig = plt.figure(figsize=(10,10)) 
fig_dims = (3, 2)

plt.subplot2grid(fig_dims, (0, 0))
data['Month'].value_counts().plot(kind='bar',title='Month')

print(data['VisitorType'].value_counts())
fig = plt.figure(figsize=(10,10)) 
fig_dims = (3, 2)

plt.subplot2grid(fig_dims, (0, 0))
data['VisitorType'].value_counts().plot(kind='bar', title='VisitorType')

data['VisitorType'].hist()

data.hist(bins = 30, figsize = (20,20), color = 'b')

data.groupby('Month')['Revenue'].value_counts().unstack('Revenue').plot(kind='bar', stacked=True, figsize=(10, 5))

data.groupby('Weekend')['Revenue'].value_counts().unstack('Revenue').plot(kind='bar', stacked=True, figsize=(7, 7))

data['VisitorType'].value_counts().plot.pie(y='VisitorType', figsize=(7, 7))

df_pvt=data[['Administrative_Duration','Informational_Duration','ProductRelated_Duration','VisitorType']]
pd.pivot_table(df_pvt, values=['Administrative_Duration','Informational_Duration','ProductRelated_Duration'],columns=['VisitorType'], aggfunc='mean').plot(kind='bar', figsize=(10, 5))

data.columns

print(data['Weekend'].value_counts())

print(data['Revenue'].value_counts())

def labelling(val):
    return f'{val / 100 * len(data):.0f}\n{val:.0f}%'
fig, (ax1) = plt.subplots(ncols=1, figsize=(10, 5))
data.groupby('Revenue').size().plot(kind='pie', autopct=labelling, textprops={'fontsize': 20},colors=['red', 'blue'], ax=ax1)
plt.show()

print(data['Revenue'].value_counts())
fig = plt.figure(figsize=(10,10)) 
fig_dims = (3, 2)

plt.subplot2grid(fig_dims, (0, 0))
data['Revenue'].value_counts().plot(kind='bar', title='Revenue')

data['VisitorType'].value_counts().plot.pie(y='VisitorType', figsize=(7, 7))

df_pvt=data[['Administrative_Duration','Informational_Duration','ProductRelated_Duration','VisitorType']]
pd.pivot_table(df_pvt, values=['Administrative_Duration','Informational_Duration','ProductRelated_Duration'],columns=['VisitorType'], aggfunc='mean').plot(kind='bar', figsize=(10, 5))

"""Apply Label encoder"""

print(data['VisitorType'].value_counts())
print(' ')
print(data['Month'].value_counts())
print(' ')
print(data['Weekend'].value_counts())
print(' ')
print(data['Revenue'].value_counts())

# Import label encoder
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

data['Month']= label_encoder.fit_transform(data['Month'])
print(data['Month'].unique())

data['VisitorType']= label_encoder.fit_transform(data['VisitorType'])
print(data['VisitorType'].unique())

data['Weekend']= label_encoder.fit_transform(data['Weekend'])
print(data['Weekend'].unique())

data['Revenue']= label_encoder.fit_transform(data['Revenue'])
print(data['Revenue'].unique())

print(data['VisitorType'].value_counts())
print(' ')
print(data['Month'].value_counts())
print(' ')
print(data['Weekend'].value_counts())
print(' ')
print(data['Revenue'].value_counts())

"""Months : 6 = MAY, 7 = NOV, 5= MARCH, 
1 = DEC,
8  = OCT,
9 = SEP,
0 = AUG,
3 = JUL,
4 = JUNE,
2 = FEB

Weekend: Fasle = 0 , True = 1

Visitor Type: Returning Visitor = 2 , New Visitor = 0 , Other = 1

Revenue: 
False = 0, True = 1
"""

data['Revenue']

false = data[data['Revenue'] == 0]
true = data[data['Revenue'] == 1]

print('Total Entrees = ', len(data))
print(' ')
print('Number of Revenue is False = ', len(false))
print('% of No Revenue = ', 1*len(false)/len(data)*100,'%')
print(' ')
print('Number of Revenue is True = ', len(true))
print('% of Revenue = ', 1*len(true)/len(data)*100,'%')

plt.figure(figsize = [10,10])

plt.subplot(413)
sns.countplot(x = 'Browser', hue = 'Revenue', data = data)

plt.subplot(414)
sns.countplot(x = 'VisitorType', hue = 'Revenue', data = data)

corr = data.corr()
plt.figure(figsize=(12,12))
sns.heatmap(corr,cbar=True,square=True,fmt='.1f',annot=True,cmap='Reds')

"""See correlation for each column against revenue
Columns with 0.0 could get rid of? OperatingSystems, Browser, Region, Traffic Type, Weekend are all very low with score of 0.0
Strong correlation is PageValues, product related and product related duration

Use precision, recall, f1 scores and roc auc curve to compare and analyse
"""

#Month={'Feb':2, 'Mar':3, 'May':5, 'Oct':10, 'June':6, 'Jul':7, 'Aug':8, 'Nov':11, 'Sep':9,'Dec':12}
#data['Month']=data['Month'].map(Month)

#VisitorType={'Returning_Visitor':3, 'New_Visitor':2, 'Other':1}
#data['VisitorType']=data['VisitorType'].map(VisitorType)
#d={True:1,False:0}
#data['Weekend']=data['Weekend'].map(d)
#data['Revenue']=data['Revenue'].map(d)

data.head(10)

Var_Corr = data.corr()
fig, ax = plt.subplots(figsize=(15,15))  
sns.heatmap(Var_Corr, xticklabels=Var_Corr.columns, yticklabels=Var_Corr.columns, annot=True)

"""## **Machine Learning**

Dataset is not balanced, so try different ways to balance it and use each for each model
"""

#remove unneccessary features and make x and y
x = data.drop(['Revenue','OperatingSystems','Browser','Region', 'TrafficType', 'Weekend'], axis=1)
y = data['Revenue']
print(x.shape)
print(y.shape)

x

"""divide into test and train"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y,train_size= 0.70, random_state = 42, stratify = y)
print(x.shape)
print(x_train.shape)
print(x_test.shape)

print('Training data has ' + str(x_train.shape[0]) + ' observation with ' + str(x_train.shape[1]) + ' features')
print('Test data has ' + str(x_test.shape[0]) + ' observation with ' + str(x_test.shape[1]) + ' features')

x.describe()

"""## **Normal**

## **Scaling**
"""

from sklearn import preprocessing

#scaler = preprocessing.StandardScaler().fit(x_train)
#print(scaler.mean_)
#print(scaler.scale_)
#X_train_scalar=scaler.transform(x_train)
#X_test_scalar=scaler.transform(x_test)

"""## **KNN**"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
y_predict = knn.predict(x_test)
knn_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of KNN = {} '.format(knn_accuracy))

from sklearn.neighbors import KNeighborsClassifier
accuracy= []
for i in range(1,25):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(x_train, y_train)
    y_predict = knn.predict(x_test)
    knn_accuracy = metrics.accuracy_score(y_test, y_predict)
    accuracy.append(knn_accuracy)
    print('Accuracy of KNN for' , i, 'neighoburs' , knn_accuracy)

print('Max: ', max(accuracy))

from sklearn.model_selection import GridSearchCV

knn_params = {
    'n_neighbors':range(1, 25) 
}
grid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid=knn_params, cv= 10, n_jobs=-1, verbose= 0 , scoring = "roc_auc", return_train_score=True)
grid_result = grid_search.fit(x_train, y_train)

print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)



knn_param_grid = {'n_neighbors': list(range(1,30)), 
                  'leaf_size': [1,10,20,30], 
                  'p': [1,2],
                  'weights': ['uniform','distance']}

knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid,verbose=3)
print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)

knn = KNeighborsClassifier(n_neighbors=24)
knn.fit(x_train, y_train)
y_predict = knn.predict(x_test)
print(classification_report(y_test, y_predict))
print( 'ROC AUC Score: ', roc_auc_score(y_test, y_predict))

knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(x_train, y_train)
y_predict = knn.predict(x_test)
print(classification_report(y_test, y_predict))
roc_auc_score(y_test, y_predict)
print( 'ROC AUC Score: ', roc_auc_score(y_test, y_predict))

knn = KNeighborsClassifier(n_neighbors=16)
knn.fit(x_train, y_train)
y_predict = knn.predict(x_test)
print(classification_report(y_test, y_predict))

knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(x_train, y_train)
y_predict = knn.predict(x_test)
cm_knn = confusion_matrix(y_test, y_predict)
print(cm_knn)

plt.figure(figsize=(7,5))
sns.heatmap(cm_knn, annot=True, fmt=".0f", linewidths=3, square=True, cmap='Blues', color="#cd1076")
plt.ylabel('actual label')
plt.xlabel('predicted label')
plt.show()

"""## **Logistic Regression**"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
log = LogisticRegression(solver='lbfgs', max_iter=1000)
log.fit(x_train, y_train)
y_predict = log.predict(x_test)
log_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of Logistic Regression = {} '.format(log_accuracy))

y_predict = log.predict(x_test)
print(classification_report(y_test, y_predict))

cm_lr = confusion_matrix(y_test, y_predict)
print(cm_lr)
plt.figure(figsize=(7,5))
sns.heatmap(cm_lr, annot=True, fmt=".0f", linewidths=3, square=True, cmap='Blues', color="#cd1076")
plt.ylabel('actual label')
plt.xlabel('predicted label')
plt.show()

# correct hyperparameter tuning for logistic regression
lr_params = {'penalty':('l1', 'l2'), 
          'C':(0.01, 0.05, 0.1, 0.5, 1, 5, 10),
          'solver': ['newton-cg', 'liblinear', 'lbfgs']}

grid_search = GridSearchCV(estimator = LogisticRegression(), param_grid=lr_params, cv= 10, n_jobs=-1, verbose= 3 , scoring = "roc_auc", return_train_score=True)
grid_result = grid_search.fit(x_train, y_train)

print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)

#with new parameters

log = LogisticRegression(C=0.1, penalty='l1', solver='liblinear')
log.fit(x_train, y_train)
y_predict = log.predict(x_test)
log_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of Logistic Regression = {} '.format(log_accuracy))
print(classification_report(y_test, y_predict))
print( 'ROC AUC Score: ', roc_auc_score(y_test, y_predict))

cm_lr = confusion_matrix(y_test, y_predict)
print(cm_lr)
plt.figure(figsize=(7,5))
sns.heatmap(cm_lr, annot=True, fmt=".0f", linewidths=3, square=True, cmap='Blues', color="#cd1076")
plt.ylabel('actual label')
plt.xlabel('predicted label')
plt.show()

"""## **Random Forest**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

randomForest = RandomForestClassifier(n_estimators= 100)
randomForest.fit(x_train, y_train)
y_predict = randomForest.predict(x_test)
forest_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of Random Forest = {} '.format(forest_accuracy))

# hyperparameter tuning for random forest
grid = {'n_estimators': [10,50,100,200],
    'max_features': ['sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy'],
    'min_samples_split': [2,5,10],
    'min_samples_leaf': [1,5,12]}

grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid=grid, cv= 5, n_jobs=-1, verbose= 3 , scoring = "roc_auc", return_train_score=True)
grid_result = grid_search.fit(x_train, y_train)

print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)

randomForest = RandomForestClassifier()

randomForest.fit(x_train, y_train)
y_predict = randomForest.predict(x_test)
forest_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of Random Forest = {} '.format(forest_accuracy))
print(classification_report(y_test, y_predict))
print( 'ROC AUC Score: ', roc_auc_score(y_test, y_predict))

cm_rF = confusion_matrix(y_test, y_predict)
print(cm_rF)
plt.figure(figsize=(7,5))
sns.heatmap(cm_rF, annot=True, fmt=".0f", linewidths=3, square=True, cmap='Blues', color="#cd1076")
plt.ylabel('actual label')
plt.xlabel('predicted label')
plt.show()

"""## **Decision Tree Classifier**"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import pydot
from IPython.display import Image

from sklearn.model_selection import train_test_split, cross_val_score
from six import StringIO 
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error,confusion_matrix, classification_report

# %matplotlib inline
plt.style.use('seaborn-white')

# This function creates images of tree models using pydot
def print_tree(estimator, features, class_names=None, filled=True):
    tree = estimator
    names = features
    color = filled
    classn = class_names
    
    dot_data = StringIO()
    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)
    graph = pydot.graph_from_dot_data(dot_data.getvalue())
    return(graph)

tree = DecisionTreeClassifier(random_state= 42)
tree.fit(x_train, y_train)
y_predict = tree.predict(x_test)
tree_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of Decision Tree = {} '.format(tree_accuracy))

# hyperparameter tuning for decision tree
params_dt = {
    'max_depth':[1,5,10,12],
    'min_samples_split':[2,4,6,8,10],
    'min_samples_leaf':list(range(1, 16)),
    'criterion': ["gini", "entropy"]
}

grid_search = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid=params_dt, cv= 5, n_jobs=-1, verbose= 0 , scoring = "roc_auc", return_train_score=True)
grid_result = grid_search.fit(x_train, y_train)
print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)

tree = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=13,min_samples_split= 4)
tree.fit(x_train, y_train)
y_predict = tree.predict(x_test)
tree_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of Decision Tree = {} '.format(tree_accuracy))

print(classification_report(y_test, y_predict))
print( 'ROC AUC Score: ', roc_auc_score(y_test, y_predict))

cm_dt = confusion_matrix(y_test, y_predict)
print(cm_dt)

"""matric and map"""

tree = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=13,min_samples_split= 4)
tree.fit(x_train, y_train)
graph2, = print_tree(tree, features=x.columns)
Image(graph2.create_png())

"""## **xgboost**"""

from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(x_train, y_train)
y_predict4 = model.predict(x_test)
xg_accuracy = metrics.accuracy_score(y_test, y_predict4)
print('Accuracy of XGBoost = {} '.format(xg_accuracy))

# hyperparameter tuning for extreme boosting tree aka xgboost
from sklearn.model_selection import GridSearchCV
params_xgb = {
    'max_features': ['sqrt', 'log2'],
    'subsample': [0.4, 0.6, 0.8],
    'max_depth': [1,5,10,15],
    'n_estimators':[10,40,60],
    'learning_rate':[0.1, 0.4, 0.8, 1.6],
        }

grid_search = GridSearchCV(estimator = XGBClassifier(), param_grid=params_xgb, cv= 5, n_jobs=-1, verbose= 3 , scoring = "roc_auc", return_train_score=True)
grid_result = grid_search.fit(x_train, y_train)
print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)

model = XGBClassifier(max_depth=5, max_features='sqrt', n_estimators=40, subsample=0.6)
model.fit(x_train, y_train)
y_predict = model.predict(x_test)
xg_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of XGBoost = {} '.format(xg_accuracy))

print(classification_report(y_test, y_predict))
print( 'ROC AUC Score: ', roc_auc_score(y_test, y_predict))

cm_xg = confusion_matrix(y_test, y_predict)
print(cm_xg)
plt.figure(figsize=(7,5))
sns.heatmap(cm_xg, annot=True, fmt=".0f", linewidths=3, square=True, cmap='Blues', color="#cd1076")
plt.ylabel('actual label')
plt.xlabel('predicted label')
plt.show()

"""heatmap and score"""

importance_df = pd.DataFrame({
    'feature': x_train.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

import seaborn as sns
plt.figure(figsize=(10,6))
plt.title('Feature Importance')
sns.barplot(data=importance_df.head(10), x='importance', y='feature');

"""## **Adboost**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC

ada = AdaBoostClassifier()
ada.fit(x_train, y_train)

y_pred = ada.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

abm_param_grid = {'n_estimators': [10,50, 120, 200], 
                  'learning_rate':[0.01,0.1,0.5,1]} 

abm_grid = GridSearchCV(AdaBoostClassifier(),param_grid=abm_param_grid,cv= 5, n_jobs=-1, verbose= 3 , scoring = "roc_auc", return_train_score=True)

grid_search = abm_grid.fit(x_train, y_train)
print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)

model = AdaBoostClassifier(learning_rate=0.1, n_estimators=200)
model.fit(x_train, y_train)
y_predict = model.predict(x_test)
xg_accuracy = metrics.accuracy_score(y_test, y_predict)
print('Accuracy of XGBoost = {} '.format(xg_accuracy))

print(classification_report(y_test, y_predict))
print( 'ROC AUC Score: ', roc_auc_score(y_test, y_predict))
cm_xg = confusion_matrix(y_test, y_predict)
print(cm_xg)

importance_df = pd.DataFrame({
    'feature': x_test.columns,
    'importance': ada.feature_importances_
}).sort_values('importance', ascending=False)

importance_df.head(10)

import seaborn as sns
plt.figure(figsize=(10,6))
plt.title('Feature Importance')
sns.barplot(data=importance_df.head(10), x='importance', y='feature');

"""## **Stacked**"""



"""## **Naive Bayes**

https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python#12.-Model-training-
"""

from sklearn.naive_bayes import GaussianNB
nbm = GaussianNB()
nbm.fit(x_train,y_train)
nbm_pred = nbm.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, nbm_pred))

"""# **Support Vector**"""

svm = SVC()
svm.fit(x_train,y_train)
svm_pred = svm.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, svm_pred))

svm_param_grid = {'C': [0.1, 1, 10, 100], 
              'gamma': [0.01, 0.1, 1, 10, 100], 
              'kernel': ['linear', 'poly', 'rbf']} 
svm_grid = GridSearchCV(SVC(),param_grid=svm_param_grid,cv= 5, n_jobs=-1, verbose= 3 , scoring = "roc_auc", return_train_score=True)

grid_search = svm_grid.fit(x_train, y_train)
print('Best Parameters: ', grid_search.best_params_)
print('Best estimators: ', grid_search.best_estimator_)

svm = SVC()
svm.fit(x_train,y_train)
svm_pred = svm.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, svm_pred))
print(classification_report(y_test, y_predict))